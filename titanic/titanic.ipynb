{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training and test data sets\n",
    "data = map(pd.read_csv, ('train.csv', 'test.csv'))\n",
    "\n",
    "def clean_data(data_set):\n",
    "    \"\"\"Cleans a data set.\"\"\"\n",
    "    # Drop columns which will not be used\n",
    "    drop_columns = ['Name', 'Ticket']\n",
    "    data_set.drop(drop_columns, axis=1, inplace=True)\n",
    "\n",
    "    # Set 'Cabin' to 1 if the passenger has a cabin, else 0\n",
    "    data_set['Cabin'] = data_set['Cabin'].notnull().astype(int)\n",
    "\n",
    "    # Convert 'Pclass' from interval [1,3] to [0,2]\n",
    "    data_set['Pclass'] -= 1\n",
    "\n",
    "    # Set string NaN values to '' and numerical to -1\n",
    "    string_columns = data_set.columns[data_set.dtypes == object]\n",
    "    data_set[string_columns] = data_set[string_columns].fillna('')\n",
    "    data_set.fillna(-1, inplace=True)\n",
    "    return data_set\n",
    "\n",
    "# Clean the data sets\n",
    "data = tuple(map(clean_data, data))\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "data = *train_test_split(data[0], test_size=0.2), data[1]\n",
    "\n",
    "def create_dataset(df):\n",
    "    \"\"\"Creates a dataset from a dataframe.\"\"\"\n",
    "    try:\n",
    "        labels = df.pop('Survived')\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "        ds = ds.shuffle(buffer_size=len(df))\n",
    "    except KeyError:\n",
    "        # Test data is not labeled\n",
    "        ds = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "    ds = ds.batch(batch_size=32)\n",
    "    return ds\n",
    "\n",
    "# Create datasets from the dataframes\n",
    "train, val, test = map(create_dataset, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature columns for the model\n",
    "feature_columns = []\n",
    "\n",
    "# Add numeric columns\n",
    "for numeric in ['Age', 'SibSp', 'Parch', 'Fare']:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(numeric))\n",
    "\n",
    "# Add one-hot encoded 'Pclass'\n",
    "pclass = tf.feature_column.categorical_column_with_identity(\n",
    "         key='Pclass', num_buckets=3)\n",
    "feature_columns.append(tf.feature_column.indicator_column(pclass))\n",
    "\n",
    "# Add one-hot encoded 'Sex'\n",
    "sex = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      key='Sex', vocabulary_list=['male', 'female'])\n",
    "feature_columns.append(tf.feature_column.indicator_column(sex))\n",
    "\n",
    "# Add one-hot encoded 'Cabin'\n",
    "cabin = tf.feature_column.categorical_column_with_identity(\n",
    "        key='Cabin', num_buckets=2)\n",
    "feature_columns.append(tf.feature_column.indicator_column(cabin))\n",
    "\n",
    "# Add one-hot encoded 'Embarked'\n",
    "embarked = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "           key='Embarked', vocabulary_list=['', 'C', 'Q', 'S'])\n",
    "feature_columns.append(tf.feature_column.indicator_column(embarked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.DenseFeatures(feature_columns),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Set up early stopping\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=5)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train, validation_data=val, epochs=100,\n",
    "          callbacks=[early_stop]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and format for submission\n",
    "predictions = np.squeeze(np.round(model.predict(test))).astype(np.uint8)\n",
    "\n",
    "# Generate submission CSV\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': data[2]['PassengerId'],\n",
    "    'Survived': predictions\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
